{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aromanenko/ATSF/blob/wip/hw3_solution_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Judging by my evaluation, this one should beat the 2nd AND 1st baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added some features, including business-based ones + tried various HistGrad models, as well as RandomForest, ended up going forward with XGBoost. Will probably try to reach a more elegant solution (for fun) over the weekends, as well as try out LightGBM - we'll see how it goes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model adds rolling and static features (I added some on top of the ones from the original notebook), uses a standard scaler, after which it applies PCA with kept variance=0.95 (inputed via CV) and then runs XGBoost (hyperparams selected using CV). Following that, a business-based guess is constructed. A blend of 70% ML, 30% Business was empirically found to be the most optimal (Believe me, I've checked, at least the percentages that can be divided by 10. Weirdly enough, with RandomForest the best blend was 20%:80%). Things that haven't been purchased in over two weeks are then cut from the data, and the negative guesses are converted to their absolute values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OCGFaLunlUEL",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy matplotlib pandas scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocSvFnH2YzNp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# %matplotlib inline\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWbdcZ5-lX52"
   },
   "source": [
    "# upload and investigate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "wMAD6a2SY81j",
    "outputId": "a34eda37-798f-42d0-b16d-43df9b6a4ce5"
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_csv('/Users/alexamarki/Desktop/projects/HW_3_2025-Markin_/train.csv', delimiter=',')\n",
    "\n",
    "# connvert date-column to data format\n",
    "all_data['period_start_dt'] = pd.to_datetime(all_data['period_start_dt'], format= \"%Y-%m-%d\")\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "-jv6Sk4Mf5Mj",
    "outputId": "117caecf-93c9-4f34-cc90-65c17f5a9dae"
   },
   "outputs": [],
   "source": [
    "# rename \"Unnamed: 0\" to id (it's needed to manage train/and)\n",
    "all_data.rename(columns={'Unnamed: 0': 'id'}, inplace=True)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "b1u_ZYGCsAiR",
    "outputId": "4f346bd5-4854-4eeb-a26f-41e8fd632740"
   },
   "outputs": [],
   "source": [
    "# investigate data holiscitly\n",
    "all_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "YpMax_hAnQmC",
    "outputId": "320dccef-8e86-4f3e-e63f-e622d42849d9"
   },
   "outputs": [],
   "source": [
    "# draw some time series\n",
    "def plot_some_ts(ts_df, groupby_columns, time_column, target_column, ts_num = 10, aggregation_method = 'sum'):\n",
    "  '''pivot original ts by group_columns\n",
    "     ts_df - original dataframe with ts data,\n",
    "     group_columns - tuple with names of columns used to split data to time series, use None if splitting is not needed\n",
    "     time_column - name of column with date, datetime64\n",
    "     target_column - column with ts data (data should be in numerical format), string\n",
    "     ts_num - number of ts to be drawn, int\n",
    "     aggregation_method - aggregation method of data in target column, string\n",
    "  '''\n",
    "\n",
    "  if groupby_columns is None:\n",
    "    ts_df[target_column + time_column + 'const'] = 1\n",
    "    groupby_columns = [target_column + time_column + 'const']\n",
    "\n",
    "  pivot_ts = ts_df.groupby(groupby_columns + [time_column]).agg(aggregation_method)\n",
    "\n",
    "  # concat multiple index to single column\n",
    "  index_column_name = ', '.join([groupby_columns[i]+'={0['+str(i)+']}' for i in range(len(groupby_columns))])\n",
    "  pivot_ts.index = [pivot_ts.index.map(index_column_name.format) , pivot_ts.index.get_level_values(len(groupby_columns))]\n",
    "\n",
    "  # unstack by-column (column that contains ts name)\n",
    "  pivot_ts = pivot_ts.unstack([0])[target_column]\n",
    "\n",
    "  # plot first ts_num ts\n",
    "  fig = pivot_ts[pivot_ts.columns[:ts_num]].plot().update_layout(height=350, width=1300,\n",
    "                                                  title=\"first {0} ts for {1} variable\".format(ts_num, target_column ),\n",
    "    xaxis_title=time_column,\n",
    "    yaxis_title=target_column+ ' value',\n",
    "    legend_title='ts id columns: '+', '.join(groupby_columns)).show()\n",
    "  return fig\n",
    "\n",
    "# data for 3 products (all stores-product level)\n",
    "plot_some_ts(all_data, ['product_rk'], 'period_start_dt', 'demand', ts_num = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "-WFLU_SLoRUM",
    "outputId": "6ea07d29-f547-412c-b12b-2925925374cc"
   },
   "outputs": [],
   "source": [
    "# all stores - all products level\n",
    "# pay attention to forecasting periods: since 2Dec2019\n",
    "plot_some_ts(all_data, None, 'period_start_dt', 'demand', ts_num = 1)\n",
    "\n",
    "# what data dependencies can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLVAHFGfiXhU",
    "outputId": "f8cff1dc-cc7b-4d22-813e-c4cd1d0e1df4"
   },
   "outputs": [],
   "source": [
    "# investigate demand driver columns (explanatory variables )\n",
    "print(all_data['PROMO1_FLAG'].unique())\n",
    "print(all_data['PROMO2_FLAG'].unique()) #не используем данную переменную, т.к. значения только 0 и NaN\n",
    "print(all_data['NUM_CONSULTANT'].unique()) #не используем данную переменную, т.к. значения только 0 и NaN\n",
    "print(all_data['AUTORIZATION_FLAG'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Y4l6rEnoif8q",
    "outputId": "aaf6b2b5-d1db-47c3-fe3f-29c0052d8af6"
   },
   "outputs": [],
   "source": [
    "# remove those, which have only one unique value (not empty) (they do not provide any benefit when training the model)\n",
    "del all_data['PROMO2_FLAG']\n",
    "del all_data['NUM_CONSULTANT']\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dauB4EmUxGG7"
   },
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4Z82zkN1spg",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## fill missing values based on expert insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "9v4WuY6yhgcZ",
    "outputId": "3a0a7c13-a5bb-468e-decd-8296dab503c5"
   },
   "outputs": [],
   "source": [
    "# fill na in PROMO1_FLAG with mode-value (based on common-sense)\n",
    "all_data['PROMO1_FLAG'] = all_data['PROMO1_FLAG'].fillna(all_data['PROMO1_FLAG'].mode()[0]) # most frequent value\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEkW7U_K9OTw",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## fill missing values with prev/back info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "id": "JblDLv84u2Zd",
    "outputId": "1712f59d-c2bb-4582-988d-2cc3fab0b952"
   },
   "outputs": [],
   "source": [
    "# AUTORIZATION_FLAG - is product available at store at the moment\n",
    "plot_some_ts(all_data, ['product_rk', 'store_location_rk'], 'period_start_dt', 'AUTORIZATION_FLAG', ts_num = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "dQg67VyN6E-Y",
    "outputId": "7f4d1a1b-b85b-4085-c56e-4a708535cd56"
   },
   "outputs": [],
   "source": [
    "# filling withh prev then next value in pandas\n",
    "all_data.set_index(['product_rk', 'store_location_rk', 'period_start_dt'])\\\n",
    "  .unstack([0,1])\\\n",
    "   ['PRICE_REGULAR'].\\\n",
    "   ffill().bfill().\\\n",
    "   stack([1,0], future_stack=True)\\\n",
    "   .rename('REGULAR_PRICE_FIXED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZX8RtK5nz_q0",
    "outputId": "20d9c638-ac24-4076-c48b-e7fa4e6dfdad"
   },
   "outputs": [],
   "source": [
    "# fill na with prev (and if no prev then next) values\n",
    "all_data = all_data.set_index(['product_rk', 'store_location_rk', 'period_start_dt']).\\\n",
    "  merge( all_data.set_index(['product_rk', 'store_location_rk', 'period_start_dt'])\\\n",
    "            .unstack([0,1])\\\n",
    "            ['PRICE_REGULAR'].\\\n",
    "              ffill().bfill().\\\n",
    "            stack([1,0],  future_stack=True).\\\n",
    "            rename('PRICE_REGULAR_FIXED'),\n",
    "         how = 'left', right_index = True, left_index = True)\\\n",
    "  .reset_index()\n",
    "\n",
    "# dell original column and replace with new one\n",
    "del all_data['PRICE_REGULAR']\n",
    "all_data.rename(columns = {'PRICE_REGULAR_FIXED':'PRICE_REGULAR'}, inplace=True)\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0fEqkKoCioy"
   },
   "outputs": [],
   "source": [
    "def ts_fillna_ffill_bfill(ts_df,column_name, ts_id):\n",
    "#  all_data.set_index(['product_rk', 'store_location_rk', 'period_start_dt'])\\ # define id columns\n",
    "#   .unstack([0,1])\\                          # df -> pivot transformation: date column - is row-index, product x store - is column index\n",
    "#   ['PRICE_REGULAR'].\\                      # define column with data to be fixed\n",
    "#   ffill().bfill().\\                        # fill missing value: apply forwand then back filling method consequently\n",
    "#   stack([1,0]).\\                           # pivot -> ts transformation\n",
    "#   rename('REGULAR_PRICE_FIXED')            # rename column\n",
    "\n",
    "\n",
    "  # fill na with prev (and if no prev then next) values\n",
    "  new_ts_df = ts_df.set_index(ts_id).\\\n",
    "    merge(ts_df.set_index(ts_id)\\\n",
    "              .unstack([0,1])\\\n",
    "              [column_name].\\\n",
    "              ffill().bfill().\\\n",
    "              stack([1,0],  future_stack=True).\\\n",
    "              rename(column_name),\n",
    "          how = 'left', right_index = True, left_index = True)\\\n",
    "    .reset_index()\n",
    "\n",
    "  # dell original column and replace with new one\n",
    "  del new_ts_df[column_name+'_x']\n",
    "  return new_ts_df.rename(columns = {column_name+'_y':column_name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6NOLzxYFE9a7",
    "outputId": "7c3b0a3e-0cee-4e8e-9de1-7cf0681e121f"
   },
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJPXFK2wAK1F"
   },
   "outputs": [],
   "source": [
    "# the same for PRICE_AFTER_DISCOUNT\n",
    "all_data = ts_fillna_ffill_bfill(\n",
    "    ts_df=all_data,\n",
    "    column_name='PRICE_AFTER_DISC',\n",
    "    ts_id=['product_rk', 'store_location_rk', 'period_start_dt']\n",
    ")\n",
    "\n",
    "# the same for AUTORIZATION_FLAG\n",
    "all_data = ts_fillna_ffill_bfill(\n",
    "    ts_df=all_data,\n",
    "    column_name='AUTORIZATION_FLAG',\n",
    "    ts_id=['product_rk', 'store_location_rk', 'period_start_dt']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "f0keVnCaFCox",
    "outputId": "e19335c1-7f60-4cfc-e79a-13c9246556be"
   },
   "outputs": [],
   "source": [
    "# look at data again\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mam5WfF9bbc",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## fill missing values as average/mode/median from other stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "gOQ_xlK29nH4",
    "outputId": "f5ee01b6-79b7-43ed-b446-93585ff3fbc4"
   },
   "outputs": [],
   "source": [
    "# find mean values for each pair product x date\n",
    "values = all_data.set_index(['product_rk', 'period_start_dt', 'store_location_rk'])\\\n",
    "            .unstack([0,1])\\\n",
    "            ['PRICE_REGULAR'].\\\n",
    "              mean()\n",
    "\n",
    "# replace missing values with mean in all stores\n",
    "all_data.set_index(['product_rk', 'store_location_rk', 'period_start_dt'])\\\n",
    "            .unstack([0,2])\\\n",
    "            ['PRICE_REGULAR'].\\\n",
    "            fillna(value = values).\\\n",
    "            stack([1,0], future_stack=True).\\\n",
    "            rename('REGULAR_PRICE_FIXED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DlhvOpTFYRA"
   },
   "outputs": [],
   "source": [
    "def ts_fillna_aggmethod(ts_df,column_name, ts_id):\n",
    "#  all_data.set_index(['product_rk', 'store_location_rk', 'period_start_dt'])\\ # define id columns\n",
    "#   .unstack([0,1])\\                          # df -> pivot transformation: date column - is row-index, product x store - is column index\n",
    "#   ['PRICE_REGULAR'].\\                      # define column with data to be fixed\n",
    "#   ffill().bfill().\\                        # fill missing value: apply forwand then back filling method consequently\n",
    "#   stack([1,0]).\\                           # pivot -> ts transformation\n",
    "#   rename('REGULAR_PRICE_FIXED')            # rename column\n",
    "\n",
    "  values = all_data.set_index(ts_id)\\\n",
    "            .unstack([0,1])\\\n",
    "            [column_name].\\\n",
    "              mean()\n",
    "\n",
    "  # fill na with prev (and if no prev then next) values\n",
    "  new_ts_df = ts_df.set_index(ts_id).\\\n",
    "    merge(ts_df.set_index(ts_id)\\\n",
    "              .unstack([0,1])\\\n",
    "              [column_name].\\\n",
    "              fillna(value = values).\\\n",
    "              stack([1,0], future_stack=True).\\\n",
    "              rename(column_name),\n",
    "          how = 'left', right_index = True, left_index = True)\\\n",
    "    .reset_index()\n",
    "\n",
    "  # dell original column and replace with new one\n",
    "  del new_ts_df[column_name+'_x']\n",
    "  return new_ts_df.rename(columns = {column_name+'_y':column_name})\n",
    "\n",
    "\n",
    "# for PRICE_REGULAR\n",
    "all_data = ts_fillna_aggmethod(\n",
    "    ts_df=all_data,\n",
    "    column_name='PRICE_REGULAR',\n",
    "    ts_id=['product_rk', 'period_start_dt', 'store_location_rk']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6Hk6osLF_QQ"
   },
   "outputs": [],
   "source": [
    "# the same for PRICE_AFTER_DISCOUNT\n",
    "all_data = ts_fillna_aggmethod(\n",
    "    ts_df=all_data,\n",
    "    column_name='PRICE_AFTER_DISC',\n",
    "    ts_id=['product_rk', 'period_start_dt', 'store_location_rk']\n",
    ")\n",
    "\n",
    "# the same for AUTORIZATION_FLAG\n",
    "all_data = ts_fillna_aggmethod(\n",
    "    ts_df=all_data,\n",
    "    column_name='AUTORIZATION_FLAG',\n",
    "    ts_id=['product_rk', 'period_start_dt', 'store_location_rk']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "uds4OkoCGxGW",
    "outputId": "6960092b-7c0c-4f4c-f440-7c146cbc5396"
   },
   "outputs": [],
   "source": [
    "# check data again\n",
    "all_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "d77k4XgXmJiG",
    "outputId": "8599a569-bbd2-4e43-ddb5-bf59ea977849"
   },
   "outputs": [],
   "source": [
    "# let's delete store 309 related data\n",
    "all_data = all_data[all_data['store_location_rk'] != 309]\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "2KaGBSLeHH0C",
    "outputId": "f512203f-13ba-4ae8-c6fc-0d54883701dc"
   },
   "outputs": [],
   "source": [
    "# check data again\n",
    "# that's it\n",
    "all_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zA6PkBIwf70",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## add calendar-feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qZ3ut5uSyXFp",
    "outputId": "d5637cd9-3b98-42a5-c699-9e6696122c4e"
   },
   "outputs": [],
   "source": [
    "all_data[\"ind_of_year\"] = [dt.year for dt in all_data.period_start_dt]\n",
    "all_data[\"ind_of_month\"] = [dt.month for dt in all_data.period_start_dt]\n",
    "all_data[\"ind_of_day\"] = [dt.day for dt in all_data.period_start_dt]\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qb8g4TQj3gmm"
   },
   "outputs": [],
   "source": [
    "# сортируем данные по ключам ряда\n",
    "all_data = all_data.sort_values(['product_rk', 'store_location_rk', 'period_start_dt'])\n",
    "\n",
    "# ---- ручные лаги спроса ----\n",
    "for lag in [1, 2, 3, 4, 7, 14, 28, 52]:\n",
    "    all_data[f'demand_lag_{lag}'] = (\n",
    "        all_data\n",
    "        .groupby(['product_rk', 'store_location_rk'])['demand']\n",
    "        .shift(lag)\n",
    "    )\n",
    "\n",
    "# ---- скользящие средние по спросу ----\n",
    "for window in [4, 8, 12, 26]:\n",
    "    all_data[f'demand_roll_mean_{window}'] = (\n",
    "        all_data\n",
    "        .groupby(['product_rk', 'store_location_rk'], group_keys=False)['demand']\n",
    "        .apply(lambda s: s.shift(1).rolling(window, min_periods=1).mean())\n",
    "    )\n",
    "\n",
    "# ---- промо/ценовые фичи ----\n",
    "all_data['discount_abs'] = all_data['PRICE_REGULAR'] - all_data['PRICE_AFTER_DISC']\n",
    "all_data['discount_rel'] = all_data['discount_abs'] / all_data['PRICE_REGULAR']\n",
    "all_data['discount_rel'] = all_data['discount_rel'].replace([np.inf, -np.inf], 0).fillna(0)\n",
    "\n",
    "all_data['is_promo'] = (all_data['PRICE_AFTER_DISC'] < all_data['PRICE_REGULAR']).astype(int)\n",
    "\n",
    "# лаг промо на 1 период\n",
    "all_data['is_promo_lag_1'] = (\n",
    "    all_data\n",
    "    .groupby(['product_rk', 'store_location_rk'])['is_promo']\n",
    "    .shift(1)\n",
    ")\n",
    "all_data['is_promo_lag_1'] = all_data['is_promo_lag_1'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PG27PHUn31OE"
   },
   "outputs": [],
   "source": [
    "# финальная чистка признаков после всех фич\n",
    "all_data = all_data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "feature_cols = [c for c in all_data.columns\n",
    "                if c not in ['id', 'demand', 'period_start_dt']]\n",
    "\n",
    "all_data[feature_cols] = all_data[feature_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eztj-yBCH1uM"
   },
   "source": [
    "# train ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eztj-yBCH1uM"
   },
   "source": [
    "## Adding more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gdv9rdEB3XGk"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from ipywidgets import IntProgress\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def percentile(n):\n",
    "    '''Calculate n - percentile of data'''\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'pctl%s' % n\n",
    "    return percentile_\n",
    "\n",
    "\n",
    "# add missing dates to GroupBy.Core object\n",
    "def fill_missing_dates(x, date_col, freq=None, default_value=np.nan):\n",
    "    if freq is None:\n",
    "        try:\n",
    "            freq = pd.infer_freq(\n",
    "                x.set_index(date_col).index[:min(100, x.shape[0])]\n",
    "            )\n",
    "        except:\n",
    "            freq = 'D'\n",
    "\n",
    "        if freq is None:\n",
    "            freq = 'D'\n",
    "            Warning('TS freq is not defined! Daily granularity is provided!')\n",
    "\n",
    "    idx = pd.date_range(x[date_col].min(), x[date_col].max(), freq=freq)\n",
    "    res = x.set_index(date_col).reindex(idx, fill_value=default_value)\n",
    "    res.index.rename(date_col, inplace=True)\n",
    "    return res.reset_index()\n",
    "\n",
    "\n",
    "def calc_preag_fill(data, group_col, date_col, target_cols, preagg_method):\n",
    "    data_preag = data.groupby(group_col).agg(preagg_method)[target_cols].reset_index()\n",
    "    data_preag_filled = data_preag.groupby(group_col[:-1]).apply(\n",
    "        fill_missing_dates, date_col=date_col\n",
    "    ).drop(group_col[:-1], axis=1).reset_index()\n",
    "    return data_preag_filled\n",
    "\n",
    "\n",
    "def calc_rolling(data_preag_filled, group_col, date_col, method, w):\n",
    "    lf_df_filled = data_preag_filled.groupby(group_col[:-1]).apply(\n",
    "        lambda x: x.set_index(date_col)\n",
    "        .rolling(window=w, min_periods=1)\n",
    "        .agg(method)\n",
    "    ).drop(group_col[:-1], axis=1).reset_index(group_col)\n",
    "    return lf_df_filled\n",
    "\n",
    "\n",
    "def calc_ewm(data_preag_filled, group_col, date_col, span):\n",
    "    lf_df_filled = data_preag_filled.groupby(group_col[:-1]).apply(\n",
    "        lambda x: x.set_index(date_col).ewm(span=span).mean()\n",
    "    ).drop(group_col[:-1], axis=1).reset_index(group_col)\n",
    "    return lf_df_filled\n",
    "\n",
    "\n",
    "def shift(lf_df_filled, group_col, date_col, lag, kwargs=None):\n",
    "    lf_df = (\n",
    "        lf_df_filled\n",
    "        .set_index(date_col)\n",
    "        .groupby(group_col[:-1])\n",
    "        .apply(lambda x: x.shift(lag, None))\n",
    "        .drop(group_col[:-1], axis=1)\n",
    "        .reset_index()\n",
    "    )\n",
    "    return lf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_static_features(out_df, target_cols, id_cols, date_col):\n",
    "    \"\"\"\n",
    "    here iadd:\n",
    "    - prod_* (by SKU)\n",
    "    - store_* (by Store)\n",
    "    - ps_* (by SKU–Store)\n",
    "    - calendar + december features\n",
    "    - last-year december (same week, avg, ratio)\n",
    "    \"\"\"\n",
    "    out_df = out_df.copy()\n",
    "    t = target_cols[0]\n",
    "    prod_col = id_cols[0]\n",
    "    store_col = id_cols[1]\n",
    "\n",
    "    # calendar\n",
    "    d = out_df[date_col]\n",
    "    out_df[\"year\"] = d.dt.year\n",
    "    out_df[\"month\"] = d.dt.month\n",
    "    try:\n",
    "        out_df[\"weekofyear\"] = d.dt.isocalendar().week.astype(int)\n",
    "    except Exception:\n",
    "        out_df[\"weekofyear\"] = d.dt.week\n",
    "\n",
    "    out_df[\"week_in_december\"] = 0\n",
    "    m_dec = out_df[\"month\"] == 12\n",
    "    tmp = out_df.loc[m_dec, [date_col, \"year\"]].copy()\n",
    "    tmp[\"week_in_december\"] = (\n",
    "        tmp.groupby(\"year\")[date_col].rank(method=\"dense\").astype(int)\n",
    "    )\n",
    "    out_df.loc[tmp.index, \"week_in_december\"] = tmp[\"week_in_december\"]\n",
    "\n",
    "    dec = out_df[out_df[\"month\"] == 12]\n",
    "\n",
    "    # product-level\n",
    "    g_prod = out_df.groupby(prod_col)[t]\n",
    "    prod = pd.DataFrame({\n",
    "        \"prod_mean\": g_prod.mean(),\n",
    "        \"prod_std\": g_prod.std(),\n",
    "        \"prod_cnt\": g_prod.count()\n",
    "    })\n",
    "\n",
    "    nz_prod = (\n",
    "        out_df.assign(_nz=(out_df[t] > 0).astype(int))\n",
    "        .groupby(prod_col)[\"_nz\"]\n",
    "        .mean()\n",
    "        .rename(\"prod_nz_share\")\n",
    "    )\n",
    "    prod = prod.join(nz_prod, how=\"left\")\n",
    "\n",
    "    if len(dec) > 0:\n",
    "        g_prod_dec = dec.groupby(prod_col)[t]\n",
    "        prod_dec_mean = g_prod_dec.mean().rename(\"prod_dec_mean\")\n",
    "        prod_dec_nz = (\n",
    "            dec.assign(_nz=(dec[t] > 0).astype(int))\n",
    "            .groupby(prod_col)[\"_nz\"]\n",
    "            .mean()\n",
    "            .rename(\"prod_dec_nz_share\")\n",
    "        )\n",
    "        prod = prod.join(prod_dec_mean, how=\"left\")\n",
    "        prod = prod.join(prod_dec_nz, how=\"left\")\n",
    "        prod[\"prod_dec_ratio\"] = (\n",
    "            prod[\"prod_dec_mean\"] / (prod[\"prod_mean\"] + 1e-6)\n",
    "        )\n",
    "\n",
    "    prod[\"prod_cv\"] = prod[\"prod_std\"] / (prod[\"prod_mean\"] + 1e-6)\n",
    "    prod = prod.reset_index()\n",
    "    out_df = out_df.merge(prod, on=prod_col, how=\"left\")\n",
    "\n",
    "    # store-level\n",
    "    g_store = out_df.groupby(store_col)[t]\n",
    "    store = pd.DataFrame({\n",
    "        \"store_mean\": g_store.mean(),\n",
    "        \"store_std\": g_store.std(),\n",
    "        \"store_cnt\": g_store.count()\n",
    "    })\n",
    "\n",
    "    nz_store = (\n",
    "        out_df.assign(_nz=(out_df[t] > 0).astype(int))\n",
    "        .groupby(store_col)[\"_nz\"]\n",
    "        .mean()\n",
    "        .rename(\"store_nz_share\")\n",
    "    )\n",
    "    store = store.join(nz_store, how=\"left\")\n",
    "\n",
    "    if len(dec) > 0:\n",
    "        g_store_dec = dec.groupby(store_col)[t]\n",
    "        store_dec_mean = g_store_dec.mean().rename(\"store_dec_mean\")\n",
    "        store_dec_nz = (\n",
    "            dec.assign(_nz=(dec[t] > 0).astype(int))\n",
    "            .groupby(store_col)[\"_nz\"]\n",
    "            .mean()\n",
    "            .rename(\"store_dec_nz_share\")\n",
    "        )\n",
    "        store = store.join(store_dec_mean, how=\"left\")\n",
    "        store = store.join(store_dec_nz, how=\"left\")\n",
    "        store[\"store_dec_ratio\"] = (\n",
    "            store[\"store_dec_mean\"] / (store[\"store_mean\"] + 1e-6)\n",
    "        )\n",
    "\n",
    "    store[\"store_cv\"] = store[\"store_std\"] / (store[\"store_mean\"] + 1e-6)\n",
    "    store = store.reset_index()\n",
    "    out_df = out_df.merge(store, on=store_col, how=\"left\")\n",
    "\n",
    "    # SKU–store pair\n",
    "    key_ps = [prod_col, store_col]\n",
    "    g_ps = out_df.groupby(key_ps)[t]\n",
    "    ps = pd.DataFrame({\n",
    "        \"ps_mean\": g_ps.mean(),\n",
    "        \"ps_std\": g_ps.std(),\n",
    "        \"ps_cnt\": g_ps.count()\n",
    "    })\n",
    "\n",
    "    nz_ps = (\n",
    "        out_df.assign(_nz=(out_df[t] > 0).astype(int))\n",
    "        .groupby(key_ps)[\"_nz\"]\n",
    "        .mean()\n",
    "        .rename(\"ps_nz_share\")\n",
    "    )\n",
    "    ps = ps.join(nz_ps, how=\"left\")\n",
    "\n",
    "    if len(dec) > 0:\n",
    "        g_ps_dec = dec.groupby(key_ps)[t]\n",
    "        ps_dec_mean = g_ps_dec.mean().rename(\"ps_dec_mean\")\n",
    "        ps_dec_nz = (\n",
    "            dec.assign(_nz=(dec[t] > 0).astype(int))\n",
    "            .groupby(key_ps)[\"_nz\"]\n",
    "            .mean()\n",
    "            .rename(\"ps_dec_nz_share\")\n",
    "        )\n",
    "        ps = ps.join(ps_dec_mean, how=\"left\")\n",
    "        ps = ps.join(ps_dec_nz, how=\"left\")\n",
    "        ps[\"ps_dec_ratio\"] = ps[\"ps_dec_mean\"] / (ps[\"ps_mean\"] + 1e-6)\n",
    "\n",
    "    ps[\"ps_cv\"] = ps[\"ps_std\"] / (ps[\"ps_mean\"] + 1e-6)\n",
    "    ps = ps.reset_index()\n",
    "    out_df = out_df.merge(ps, on=key_ps, how=\"left\")\n",
    "\n",
    "    # last-year dec by week\n",
    "    if len(dec) > 0:\n",
    "        dec_all = out_df[out_df[\"month\"] == 12].copy()\n",
    "        # year-level avg\n",
    "        dec_year = (\n",
    "            dec_all\n",
    "            .groupby(key_ps + [\"year\"])[t]\n",
    "            .mean()\n",
    "            .rename(\"dec_mean_year\")\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # week-level\n",
    "        dec_week = (\n",
    "            dec_all\n",
    "            .groupby(key_ps + [\"year\", \"week_in_december\"])[t]\n",
    "            .mean()\n",
    "            .rename(\"demand_dec_week\")\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        dec_week = dec_week.merge(\n",
    "            dec_year,\n",
    "            on=key_ps + [\"year\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        dec_week[\"year\"] = dec_week[\"year\"] + 1\n",
    "        dec_week[\"demand_last_year_same_week\"] = dec_week[\"demand_dec_week\"]\n",
    "        dec_week[\"dec_last_year_avg\"] = dec_week[\"dec_mean_year\"]\n",
    "        dec_week[\"week_ratio_last_year\"] = (\n",
    "            dec_week[\"demand_last_year_same_week\"]\n",
    "            / (dec_week[\"dec_last_year_avg\"] + 1e-6)\n",
    "        )\n",
    "\n",
    "        dec_week = dec_week[\n",
    "            key_ps\n",
    "            + [\"year\", \"week_in_december\",\n",
    "               \"demand_last_year_same_week\",\n",
    "               \"dec_last_year_avg\",\n",
    "               \"week_ratio_last_year\"]\n",
    "        ]\n",
    "\n",
    "        out_df = out_df.merge(\n",
    "            dec_week,\n",
    "            on=key_ps + [\"year\", \"week_in_december\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lagged_features(\n",
    "        data: pd.DataFrame,\n",
    "        target_cols: list = ['Demand'],\n",
    "        id_cols: list = ['SKU_id', 'Store_id'],\n",
    "        date_col: str = 'Date',\n",
    "        lags: list = [7, 14, 21, 28],\n",
    "        windows: list = ['7D', '14D', '28D', '56D'],\n",
    "        preagg_methods: list = ['mean'],\n",
    "        agg_methods: list = ['mean', 'median', percentile(10), pd.Series.skew],\n",
    "        dynamic_filters: list = ['weekday', 'Promo'],\n",
    "        ewm_params: dict = {'weekday': [14, 28], 'Promo': [14, 42]}\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    '''\n",
    "    data - dataframe with default index\n",
    "    target_cols - column names for lags calculation\n",
    "    id_cols - key columns to identify unique values\n",
    "    date_col - column with datetime format values\n",
    "    lags - lag values(days)\n",
    "    windows - windows(days/weeks/months/etc.),\n",
    "        calculation is performed within time range length of window\n",
    "    preagg_methods - applied methods before rolling to make\n",
    "        every value unique for given id_cols\n",
    "    agg_methods - method of aggregation('mean', 'median', percentile, etc.)\n",
    "    dynamic_filters - column names to use as filter\n",
    "    ewm_params - span values(days) for each dynamic_filter\n",
    "    '''\n",
    "\n",
    "    data = data.sort_values(date_col)\n",
    "    out_df = deepcopy(data)\n",
    "\n",
    "    out_df[date_col] = pd.to_datetime(out_df[date_col])\n",
    "    out_df = add_static_features(out_df, target_cols, id_cols, date_col)\n",
    "\n",
    "    # drop level_* created by groupby.apply\n",
    "    bad_cols = [c for c in out_df.columns if str(c).startswith('level_')]\n",
    "    if bad_cols:\n",
    "        out_df = out_df.drop(columns=bad_cols)\n",
    "\n",
    "    total = (\n",
    "        len(target_cols)\n",
    "        * len(lags)\n",
    "        * len(windows)\n",
    "        * len(preagg_methods)\n",
    "        * len(agg_methods)\n",
    "        * len(dynamic_filters)\n",
    "    )\n",
    "    progress = IntProgress(min=0, max=total)\n",
    "    display(progress)\n",
    "\n",
    "    for filter_col in dynamic_filters:\n",
    "        group_col = [filter_col] + id_cols + [date_col]\n",
    "\n",
    "        for preagg in preagg_methods:\n",
    "            data_preag_filled = calc_preag_fill(\n",
    "                out_df, group_col, date_col, target_cols, preagg\n",
    "            )\n",
    "\n",
    "            bad_cols = [c for c in data_preag_filled.columns\n",
    "                        if str(c).startswith('level_')]\n",
    "            if bad_cols:\n",
    "                data_preag_filled = data_preag_filled.drop(columns=bad_cols)\n",
    "\n",
    "            for alpha in ewm_params.get(filter_col, []):\n",
    "                ewm_filled = calc_ewm(\n",
    "                    data_preag_filled, group_col, date_col, alpha\n",
    "                )\n",
    "\n",
    "                bad_cols = [c for c in ewm_filled.columns\n",
    "                            if str(c).startswith('level_')]\n",
    "                if bad_cols:\n",
    "                    ewm_filled = ewm_filled.drop(columns=bad_cols)\n",
    "\n",
    "                for lag in lags:\n",
    "                    ewm = shift(ewm_filled, group_col, date_col, lag)\n",
    "                    keep_cols = [c for c in ewm.columns\n",
    "                                 if (c in group_col) or (c in target_cols)]\n",
    "                    ewm = ewm[keep_cols]\n",
    "\n",
    "                    new_names = {\n",
    "                        x: \"{0}_lag{1}d_alpha{2}_key{3}_preag{4}_{5}_dynamic_ewm\".format(\n",
    "                            x, lag, alpha, '&'.join(id_cols), preagg, filter_col\n",
    "                        )\n",
    "                        for x in target_cols\n",
    "                    }\n",
    "\n",
    "                    out_df = pd.merge(\n",
    "                        out_df,\n",
    "                        ewm.rename(columns=new_names),\n",
    "                        how='left',\n",
    "                        on=group_col\n",
    "                    )\n",
    "\n",
    "            for w in windows:\n",
    "                for method in agg_methods:\n",
    "                    rolling_filled = calc_rolling(\n",
    "                        data_preag_filled, group_col, date_col, method, w\n",
    "                    )\n",
    "\n",
    "                    bad_cols = [c for c in rolling_filled.columns\n",
    "                                if str(c).startswith('level_')]\n",
    "                    if bad_cols:\n",
    "                        rolling_filled = rolling_filled.drop(columns=bad_cols)\n",
    "\n",
    "                    for lag in lags:\n",
    "                        rolling = shift(\n",
    "                            rolling_filled, group_col, date_col, lag\n",
    "                        )\n",
    "\n",
    "                        keep_cols = [c for c in rolling.columns\n",
    "                                     if (c in group_col) or (c in target_cols)]\n",
    "                        rolling = rolling[keep_cols]\n",
    "\n",
    "                        method_name = (\n",
    "                            method.__name__\n",
    "                            if isinstance(method, str) is False else method\n",
    "                        )\n",
    "\n",
    "                        new_names = {\n",
    "                            x: \"{0}_lag{1}d_w{2}_key{3}_preag{4}_ag{5}_{6}_dynamic_rolling\".format(\n",
    "                                x, lag, w, '&'.join(id_cols), preagg,\n",
    "                                method_name, filter_col\n",
    "                            )\n",
    "                            for x in target_cols\n",
    "                        }\n",
    "\n",
    "                        out_df = pd.merge(\n",
    "                            out_df,\n",
    "                            rolling.rename(columns=new_names),\n",
    "                            how='left',\n",
    "                            on=group_col\n",
    "                        )\n",
    "                        progress.value += 1\n",
    "\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e5fc477148f949488d8899c2a061012f",
      "d459d56cdc5446d689ca25cb21dc0a90",
      "aaef7c3ec5684836ace59bc1a8afce81"
     ]
    },
    "id": "WRXr1PmjhGdy",
    "outputId": "9cfedc36-eeef-4b01-bea4-08398094174b"
   },
   "outputs": [],
   "source": [
    "target_cols = ['demand']\n",
    "id_cols = ['product_rk', 'store_location_rk']\n",
    "date_col = 'period_start_dt'\n",
    "built_in_funcs = [pd.Series.kurtosis, pd.Series.skew]\n",
    "all_data['NoFilter'] = 1\n",
    "all_data[\"period_start_dt\"] = pd.to_datetime(all_data[\"period_start_dt\"])\n",
    "\n",
    "\n",
    "all_data = generate_lagged_features(\n",
    "    all_data,\n",
    "    target_cols=target_cols,\n",
    "    id_cols=id_cols,\n",
    "    date_col=date_col,\n",
    "    lags=[7, 14, 21, 28],                    \n",
    "    windows=['7D', '14D', '28D', '56D'],     \n",
    "    preagg_methods=['mean'],                 \n",
    "    agg_methods=['mean', 'median', percentile(10), percentile(90)],\n",
    "    dynamic_filters=['NoFilter'],\n",
    "    ewm_params={'NoFilter': [14, 28, 56]}   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "3AkOFpWoZGaZ",
    "outputId": "8d7c6e53-611c-48eb-dd3d-1676cb27d733"
   },
   "outputs": [],
   "source": [
    "#split train and test data\n",
    "data_train = all_data[all_data['demand'].isnull() == False]\n",
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "0QlFFYUKZpjl",
    "outputId": "4cb1e767-9e7b-4308-b382-b88dcf78a679"
   },
   "outputs": [],
   "source": [
    "data_test = all_data[all_data['demand'].isnull()]\n",
    "# rename \"demand\" to \"predicted\"\n",
    "data_test.rename(columns={'demand': 'predicted'}, inplace=True)\n",
    "data_test # 1200 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bBMDL7i82dE"
   },
   "outputs": [],
   "source": [
    "X = data_train.drop(['id', 'demand', 'period_start_dt'], axis=1)\n",
    "y = data_train['demand']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "5ddrS9DO9c3b",
    "outputId": "ad16ab3c-c140-46e3-829f-cee8cf0479e6"
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "YQnIPnAx9dnP",
    "outputId": "bb5a9ab9-1fa3-4465-ffd0-1e9f3d83eca6"
   },
   "outputs": [],
   "source": [
    "# answers in train period\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kk5BlLsjjXmm"
   },
   "outputs": [],
   "source": [
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "X = X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gXUiyPl5qEC"
   },
   "outputs": [],
   "source": [
    "# for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i included a \"divisor\" variable that is large enough for the values to be negligeble enough for the proper functioning of log1p and expm1, as the work properly when 1 = x+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOAtaf7sX4zF"
   },
   "outputs": [],
   "source": [
    "divisor = max(y_train) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csPu9txhYK6r"
   },
   "outputs": [],
   "source": [
    "y_train = y_train / divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LldJH-SqRiaS"
   },
   "outputs": [],
   "source": [
    "y_train = np.log1p(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7KOVscHtz3OL",
    "outputId": "b3e2ac30-cad7-4a6f-dac9-e571263af7a2"
   },
   "outputs": [],
   "source": [
    "!pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hx5RdJAR3_-Z",
    "outputId": "e902b7df-dcb3-44a2-807d-8efb03624872"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, SplineTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from xgboost import XGBRegressor  \n",
    "\n",
    "all_feats = X_train.columns\n",
    "\n",
    "regressor = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),         \n",
    "    (\"pca\", PCA(n_components=0.99)),      \n",
    "    (\"model\", XGBRegressor(\n",
    "        n_estimators=500,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=1.0,\n",
    "        reg_lambda=1.0,\n",
    "        objective=\"reg:squarederror\",\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\"        \n",
    "    ))\n",
    "])\n",
    "\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3GwJrTkXJKzu"
   },
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "y_pred = np.expm1(y_pred)\n",
    "y_pred = y_pred * divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAThlsmU6BFg"
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E9_VfuS-6Fmx"
   },
   "outputs": [],
   "source": [
    "# score sample from forecasting period\n",
    "X_test_sub = data_test.drop(['id', 'predicted', 'period_start_dt'], axis=1)\n",
    "\n",
    "# clear of infs\n",
    "X_test_sub = X_test_sub.replace([np.inf, -np.inf], np.nan).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJUENfnlynLI"
   },
   "outputs": [],
   "source": [
    "y_pred_res = regressor.predict(X_test_sub)\n",
    "y_pred_res = np.expm1(y_pred_res)\n",
    "y_pred_res = y_pred_res * divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1t8faMIJzCts"
   },
   "outputs": [],
   "source": [
    "y_results = data_test[['id', 'product_rk', 'store_location_rk', 'period_start_dt']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQk9JDLudWix"
   },
   "outputs": [],
   "source": [
    "y_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forecast preprocessing + business-informed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The ML model on its own isn't quite perfect at guessing, and neither are algorithmic methods. However, a combination of the two yields better results (i checked by hand), hence I'm going to add a data-based guess:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cut the stuff that wasn't bought in the last 2 weeks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 14\n",
    "cut_data = data_train[data_train['period_start_dt'] >= data_train['period_start_dt'].max() - pd.Timedelta(days=cut)]\n",
    "to_cut = cut_data.groupby(['product_rk', 'store_location_rk'])['demand'].sum()\n",
    "to_cut = to_cut[to_cut == 0].reset_index()[['product_rk', 'store_location_rk']]\n",
    "to_cut['cut'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are guessing values in the late 2019, which is why it makes the most sense to impute them using november and december data of the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_niwOH0UzpU5"
   },
   "outputs": [],
   "source": [
    "def get_week(date):\n",
    "    if date.day < 8: \n",
    "        return 1\n",
    "    elif date.day < 15: \n",
    "        return 2\n",
    "    elif date.day < 22: \n",
    "        return 3\n",
    "    elif date.day <= 29: \n",
    "        return 4\n",
    "    else: \n",
    "        return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vaR_6U-pzE8U"
   },
   "outputs": [],
   "source": [
    "y_results['week_for_month'] = y_results['period_start_dt'].apply(get_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "td6S8aWdzl_i"
   },
   "outputs": [],
   "source": [
    "dec2018 = data_train[np.logical_and('2018-12-01' <= data_train['period_start_dt'], data_train['period_start_dt'] <= '2018-12-31')]\n",
    "\n",
    "dec2018_c1 = dec2018.copy()\n",
    "dec2018_c2 = dec2018.copy()\n",
    "\n",
    "dec2018_c1['week_for_month'] = dec2018_c1['period_start_dt'].apply(get_week)\n",
    "w2w = dec2018_c1.groupby(['product_rk', 'week_for_month'])['demand'].mean().reset_index()\n",
    "m2m = dec2018_c1.groupby('product_rk')['demand'].mean().reset_index()\n",
    "m2m.rename(columns={'demand': 'm2m'}, inplace=True)\n",
    "w2w = w2w.merge(m2m, on='product_rk')\n",
    "w2w['w2w'] = w2w['demand'] / w2w['m2m']\n",
    "\n",
    "dec2018_av = dec2018_c2.groupby(['product_rk', 'store_location_rk'])['demand'].mean().reset_index()\n",
    "dec2018_av.rename(columns={'demand': 'dec2018_av'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8dUM57Az02x"
   },
   "outputs": [],
   "source": [
    "nov2018 = data_train[np.logical_and('2018-11-01' <= data_train['period_start_dt'], data_train['period_start_dt'] <= '2018-11-30')]\n",
    "nov2019 = data_train[np.logical_and('2019-11-01' <= data_train['period_start_dt'], data_train['period_start_dt'] <= '2019-11-30')]\n",
    "\n",
    "nov2018 = nov2018.groupby('product_rk')['demand'].sum().reset_index()\n",
    "nov2018.rename(columns={'demand': 'nov2018'}, inplace=True)\n",
    "nov2019 = nov2019.groupby('product_rk')['demand'].sum().reset_index()\n",
    "nov2019.rename(columns={'demand': 'nov2019'}, inplace=True)\n",
    "\n",
    "y2y = nov2018.merge(nov2019, on='product_rk', how='inner')\n",
    "y2y['y2y'] = y2y['nov2019'] / y2y['nov2018']\n",
    "y2y['y2y'] = y2y['y2y'].clip(0.5, 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZZhdPzGzGe9"
   },
   "outputs": [],
   "source": [
    "y_results = y_results.merge(to_cut, on=['product_rk', 'store_location_rk'], how='left')\n",
    "y_results = y_results.merge(w2w[['product_rk', 'w2w', 'week_for_month']],\n",
    "                           on=['product_rk', 'week_for_month'], how='left')\n",
    "y_results = y_results.merge(dec2018_av, on=['product_rk', 'store_location_rk'], how='left')\n",
    "y_results = y_results.merge(y2y[['product_rk', 'y2y']], on='product_rk', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R4jHju186x_J"
   },
   "outputs": [],
   "source": [
    "y_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIz6gFFhzHrZ"
   },
   "outputs": [],
   "source": [
    "y_results['cut'] = y_results['cut'].fillna(1)\n",
    "y_results['w2w'] = y_results['w2w'].fillna(1.0)\n",
    "y_results['dec2018_av'] = y_results['dec2018_av'].fillna(y_results['dec2018_av'].mean())\n",
    "y_results['y2y'] = y_results['y2y'].fillna(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWFSdJppzJKv"
   },
   "outputs": [],
   "source": [
    "y_results['predicted_business'] = y_results['cut'] * y_results['w2w'] * y_results['dec2018_av'] * y_results['y2y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mrKUVWvyfn6o"
   },
   "outputs": [],
   "source": [
    "y_pred_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FXezkj8Ffs5X"
   },
   "outputs": [],
   "source": [
    "y_results['predicted_business']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUxrhTpTzKr4"
   },
   "outputs": [],
   "source": [
    "ml_model_coef = 0.7\n",
    "predicted_business_coef = 1 - ml_model_coef\n",
    "y_results['fin_pred'] = (y_pred_res * ml_model_coef + predicted_business_coef * y_results['predicted_business'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vV20K_JsfxFB"
   },
   "outputs": [],
   "source": [
    "y_results['fin_pred']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDyitLJP2qsC"
   },
   "source": [
    "# forecast postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsKv0O74f5Oe"
   },
   "outputs": [],
   "source": [
    "y_results['cut'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2t00e8HzMOR"
   },
   "outputs": [],
   "source": [
    "y_results.loc[y_results['cut'] == 0, 'fin_pred'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YwIv7cC9h8cV"
   },
   "outputs": [],
   "source": [
    "y_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2iOSFau1kJtk"
   },
   "outputs": [],
   "source": [
    "# extending the y_train value a little bit for some clipping boundary but not a lot of limitations (if i don't clip, the results are worse; if i only clip it using * divisor, the results are also worse)\n",
    "y_train = np.expm1(y_train) * divisor * divisor\n",
    "clipping = np.percentile(y_train, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpcUW6vdzPXL"
   },
   "outputs": [],
   "source": [
    "y_results['fin_pred_FIN'] = np.clip(y_results['fin_pred'], 0, clipping)\n",
    "y_results['fin_pred_FIN'] = np.round(y_results['fin_pred_FIN']) # rounding, also leads to a better SMAPE score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uEw_SmviAL3"
   },
   "outputs": [],
   "source": [
    "y_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpQcgEF7zZ65"
   },
   "outputs": [],
   "source": [
    "y_results.loc[y_results['fin_pred_FIN'] < 0, 'fin_pred_FIN'] = 0\n",
    "# mirror negative, if any persist (don't think that's likely, but okay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4I5KczsHiEfP"
   },
   "outputs": [],
   "source": [
    "y_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndRn_KKyza7n"
   },
   "outputs": [],
   "source": [
    "y_results['predicted'] = y_results['fin_pred_FIN']\n",
    "results_fin = y_results[['id', 'predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b93X42hGiFy5"
   },
   "outputs": [],
   "source": [
    "y_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sz32rauuzb-u"
   },
   "outputs": [],
   "source": [
    "results_fin.to_csv('./submission_example.csv', sep=',', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "aaef7c3ec5684836ace59bc1a8afce81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d459d56cdc5446d689ca25cb21dc0a90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5fc477148f949488d8899c2a061012f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d459d56cdc5446d689ca25cb21dc0a90",
      "max": 64,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_aaef7c3ec5684836ace59bc1a8afce81",
      "value": 64
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
